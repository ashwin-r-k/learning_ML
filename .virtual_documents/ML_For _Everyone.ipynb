


import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler


 get_ipython().getoutput("wget https://raw.githubusercontent.com/ashwin-r-k/learning_ML/master/magic04.data")


from imblearn.over_sampling import RandomOverSampler


cols = ["fLength","fWidth","fSize", "fConc", "fConcl", "fAsym", "fMllong", "fX3Trans", "fAlpha",  "fDist", "class"]
df = pd.read_csv("magic04.data",names=cols)


df["class"] = (df["class"]=="g").astype(int)


print(df)


df.head()


for label in cols[:-1]:
    plt.hist( df[ df["class"]==1 ][label], color="blue" , label="gamma" , alpha = 0.7,density=True  )
    plt.hist( df[ df["class"]==0 ][label], color="red" , label="hadron" , alpha = 0.7,density=True  )
    plt.title(label)
    plt.xlabel(label)
    plt.ylabel("probability")
    plt.legend()
    plt.show()





train, valid, test = np.split( df.sample(frac=1) , [int(0.6*len(df)), int(0.8*len(df)) ] )


# how we have to scale the data to get propper result else it will beprint( not accurete predication
print(train)


def scaledataset(dataframe,oversample=False):
    X = dataframe[dataframe.columns[:-1]].values
    y = dataframe[dataframe.columns[-1]].values

    scaler = StandardScaler()
    x = scaler.fit_transform(X)

    if oversample:
      ros = RandomOverSampler()
      X, y = ros.fit_resample(X,y)

    data = np.hstack((X, np.reshape(y,(-1,1) )))
        #-1 means the len(y)
    return data, X, y


print(len(train [ train["class"] == 1])) # Gamas
print(len(train [ train["class"] == 0])) #Hadrons

# So now we have to over sample the data as the numbers of Hadron data is very less


train, X_train, y_train = scaledataset(train,oversample=True)
test, x_test, y_test = scaledataset(test,oversample=False)
valid, x_valid, y_valid = scaledataset(valid,oversample=False)


# KNN

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report



knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train,y_train)



y_predict = knn_model.predict(x_test)



y_predict


y_test


print(classification_report(y_test,y_predict))


print(classification_report(y_test,y_predict))


print(classification_report(y_test,y_predict))





from sklearn.naive_bayes import GaussianNB


nb_model = GaussianNB()


nb_model.fit(X_train,y_train)


y_predict = nb_model.predict(x_test)


print(classification_report(y_test,y_predict))





from sklearn.svm import SVC


# START_CELL_MAGIC("time", "")
svm_model = SVC()
svm_model = svm_model.fit(X_train,y_train)
# END_CELL_MAGIC


y_predict = svm_model.predict(x_test)


print(classification_report(y_test,y_predict))





import tensorflow as tf



def plot_loss_accuracy(history):
    fig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,6))

    ax1.plot(history.history['loss'],label='loss' )
    ax1.plot(history.history['val_loss'],label='val_loss')
    ax1.set_ylabel('Binary crossentropy')
    ax1.set_xlabel('Epoch')


    ax2.plot(history.history['accuracy'],label='accuracy')
    ax2.plot(history.history['val_accuracy'],label='accuracy')
    ax2.set_ylabel('Accuracy')
    ax2.set_xlabel('Epoch')

    ax1.legend()
    ax1.grid(True)
    ax2.legend()
    ax2.grid(True)

    plt.show()


nn_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32,activation="relu"),
    tf.keras.layers.Dense(32,activation="relu"),
    tf.keras.layers.Dense(1,activation="sigmoid")
    ])

nn_model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss="binary_crossentropy", metrics=['accuracy'])


# START_CELL_MAGIC("time", "")
history = nn_model.fit(
    X_train,y_train,epochs=100,batch_size=32,validation_split=0.2
)
# END_CELL_MAGIC


plot_loss_accuracy(history)


from tqdm.keras import TqdmCallback
def trainmodel(X_train,y_train,num_nodes,dropout_prob,lr,batch_size,epochs):
    nn_model = tf.keras.Sequential([
    tf.keras.layers.Dense(num_nodes,activation="relu"),
    tf.keras.layers.Dropout(dropout_prob),
    tf.keras.layers.Dense(num_nodes,activation="relu"),
    tf.keras.layers.Dropout(dropout_prob),
    tf.keras.layers.Dense(1,activation="sigmoid")

    ])

    nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss="binary_crossentropy", metrics=['accuracy'] )

    history = nn_model.fit(
        X_train,y_train,epochs=epochs,batch_size=batch_size,validation_split=0.2,verbose=0, callbacks=[TqdmCallback(verbose=1)]
    )
    return nn_model, history



# START_CELL_MAGIC("time", "")
epochs=100
num_nodes,dropout_prob,lr,batch_size = 64,0.2,0.001,64
model, history = trainmodel(X_train,y_train,num_nodes,dropout_prob,lr,batch_size,epochs)
plot_loss_accuracy(history)
# END_CELL_MAGIC


# START_CELL_MAGIC("time", "")
least_val_loss = float('inf')
least_nn_model = None

epochs=50

for num_nodes in  [8,16,32]:
    for dropout_prob in [0 , 0.2]:
        for lr in  [0.01, 0.005, 0.001]:
            for batch_size in [32, 64]:
                print(f'Batch size: {batch_size} , lr: {lr}, dropout prob: {dropout_prob}, numnode: {num_nodes} ')
                model, history = trainmodel(X_train,y_train,num_nodes,dropout_prob,lr,batch_size,epochs)
                #plot_loss_accuracy(history)

                val_loss = model.evaluate(x_valid,y_valid )[0]
                if val_loss < least_val_loss:
                    least_val_loss=val_loss
                    least_nn_model=model







# END_CELL_MAGIC


y_predict = least_nn_model.predict(x_valid)
print(y_predict)
y_predict = (y_predict>0.5).astype('int')
print(y_predict)


y_predict = (y_predict>0.5).astype('int')
print(y_predict)


print(classification_report(y_test,y_predict))


y_predict = model.predict(x_test)
print(y_predict)
y_predict = (y_predict>0.5).astype('int').reshape(-1)
print(y_predict)
print(classification_report(y_test,y_predict))



